{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from llama_index.core.schema import TransformComponent\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import TitleExtractor, SummaryExtractor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "#huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "#embed_model = LangchainEmbedding(huggingface_embeddings)\n",
    "\n",
    "# Define a custom transformation component\n",
    "class CustomTransformation(TransformComponent):\n",
    "    def __call__(self, nodes, **kwargs):\n",
    "        for node in nodes:\n",
    "            node.text = node.text.lower()\n",
    "            node.text = re.sub(r'\\s+', ' ', node.text)  # Replace multiple spaces with a single space\n",
    "            node.text = re.sub(r'[^\\w\\s]', '', node.text)  # Removes punctuation\n",
    "        return nodes\n",
    "\n",
    "# # Define the embedding model transformation component\n",
    "# class EmbeddingModel(TransformComponent):\n",
    "#     def __init__(self):\n",
    "#         self.model = embed_model\n",
    "\n",
    "#     def __call__(self, nodes):\n",
    "#         for node in nodes:\n",
    "#             node.embedding = self.model.get_text_embedding(node.text)\n",
    "#         return nodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "def get_embedding_model(embedding_model_name, embed_batch_size):\n",
    "    embedding_model = HuggingFaceEmbedding(\n",
    "            model_name=embedding_model_name,\n",
    "            embed_batch_size=embed_batch_size\n",
    "        )\n",
    "    return embedding_model\n",
    "\n",
    "class EmbedModel(TransformComponent):\n",
    "    embedding_model: object = Field(default=None, exclude=True)\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            embed_batch_size=100\n",
    "        )\n",
    "\n",
    "    def __call__(self, nodes: List[object]) -> List[object]:\n",
    "        for node in nodes:\n",
    "            node.embedding = self.embedding_model.get_text_embedding(node.text)\n",
    "        return nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentence_Splitter_docs_into_nodes(all_documents):\n",
    "    try:\n",
    "        splitter = SentenceSplitter(\n",
    "            chunk_size=1500,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "\n",
    "        nodes = splitter.get_nodes_from_documents(all_documents)\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents into nodes: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sentence_Splitter_docs_into_nodes() missing 1 required positional argument: 'all_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the ingestion pipeline\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(\n\u001b[0;32m      3\u001b[0m     transformations\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         CustomTransformation(),\n\u001b[1;32m----> 5\u001b[0m         \u001b[43mSentence_Splitter_docs_into_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      6\u001b[0m         EmbedModel(),\n\u001b[0;32m      7\u001b[0m     ]\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Sentence_Splitter_docs_into_nodes() missing 1 required positional argument: 'all_documents'"
     ]
    }
   ],
   "source": [
    "# Create the ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        CustomTransformation(),\n",
    "        Sentence_Splitter_docs_into_nodes(),\n",
    "        EmbedModel(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 5/5 [00:09<00:00,  1.87s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'show_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(transformations\u001b[38;5;241m=\u001b[39m[CustomTransformation(), SentenceSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m),EmbedModel()])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Run the ingestion pipeline\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m nodes_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(nodes_parsed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\ingestion\\pipeline.py:567\u001b[0m, in \u001b[0;36mIngestionPipeline.run\u001b[1;34m(self, show_progress, documents, nodes, cache_collection, in_place, store_doc_text, num_workers, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y, nodes_parallel, [])\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43mrun_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_place\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_place\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39madd([n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\ingestion\\pipeline.py:122\u001b[0m, in \u001b[0;36mrun_transformations\u001b[1;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m cached_nodes\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m         cache\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;28mhash\u001b[39m, nodes, collection\u001b[38;5;241m=\u001b[39mcache_collection)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:221\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;129m@wrapt\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(func, instance, args, kwargs):\n\u001b[1;32m--> 221\u001b[0m     bound_args \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     id_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     token \u001b[38;5;241m=\u001b[39m active_span_id\u001b[38;5;241m.\u001b[39mset(id_)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:3211\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:3200\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3198\u001b[0m         arguments[kwargs_param\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m   3199\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3200\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3201\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   3202\u001b[0m                 arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[0;32m   3204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[1;31mTypeError\u001b[0m: got an unexpected keyword argument 'show_progress'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load data from directory\n",
    "\n",
    "        #reader = SimpleDirectoryReader(input_dir=r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\")\n",
    "        documents = SimpleDirectoryReader(input_dir=r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\").load_data(show_progress = True)\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "        if documents:\n",
    "            documents = CustomTransformation(documents)\n",
    "\n",
    "            # Split documents into nodes\n",
    "            nodes = Sentence_Splitter_docs_into_nodes(documents)\n",
    "\n",
    "            # Initialize embedding model\n",
    "            embeddings = EmbedModel(nodes)\n",
    "        else:\n",
    "            print(\"No documents to process.\")\n",
    "\n",
    "        # Run the ingestion pipeline\n",
    "        #nodes_parsed = pipeline.run(documents=documents)\n",
    "        print(f\"Created {len(embeddings)} nodes\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqdrant_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QdrantClient\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from uuid import uuid4\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentence_Splitter_docs_into_nodes(all_documents):\n",
    "    \"\"\"\n",
    "    Splits the documents into nodes using a sentence splitter.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        splitter = SentenceSplitter(\n",
    "            chunk_size=1500,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "\n",
    "        nodes = splitter.get_nodes_from_documents(all_documents)\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents into nodes: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from uuid import uuid4\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def extract_metadata_from_pdf_and_process_nodes(pdf_path, nodes):\n",
    "    \"\"\"\n",
    "    Extract metadata from a PDF and process the nodes with metadata.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    pdf_path (str): The path to the PDF file.\n",
    "    nodes (list): The list of document nodes.\n",
    "    client (QdrantClient): The Qdrant client instance.\n",
    "    collection_name (str): The name of the collection.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = document.metadata\n",
    "    \n",
    "    # Prepare the chunked metadata list\n",
    "    chunked_metadata = []\n",
    "    \n",
    "    for item in nodes:\n",
    "        content = item['content']  # Assume each node has 'content'\n",
    "        source = item['metadata']['source']\n",
    "        page = item['metadata']['page']\n",
    "        \n",
    "        id = str(uuid4())\n",
    "\n",
    "        payload = {\n",
    "           \"page_content\": content,\n",
    "           \"metadata\": {\n",
    "                        \"id\": id,\n",
    "                        \"page_content\": content,\n",
    "                        \"source\": source,\n",
    "                        \"page\": page,\n",
    "                        \"Title\": metadata.get('title', 'N/A'),\n",
    "                        \"Author\": metadata.get('author', 'N/A'),\n",
    "                        \"CreationDate\": metadata.get('creationDate', 'N/A'),\n",
    "                        }\n",
    "            }\n",
    "\n",
    "        metadata_struct = PointStruct(id=id, payload=payload)\n",
    "        chunked_metadata.append(metadata_struct)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_metadata(data):\n",
    "    \"\"\"\n",
    "    Process and upsert chunked metadata into Qdrant.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    data (list): The list of document chunks.\n",
    "    client (QdrantClient): The Qdrant client instance.\n",
    "    collection_name (str): The name of the collection.\n",
    "\n",
    "    \"\"\"\n",
    "    chunked_metadata = []\n",
    "\n",
    "    for item in data:\n",
    "        content = item.page_content\n",
    "\n",
    "        id = str(uuid4())\n",
    "        source = item.metadata[\"source\"]\n",
    "        page = item.metadata[\"page\"]\n",
    "\n",
    "        payload = {\n",
    "           \"page_content\": content,\n",
    "           \"metadata\": {\n",
    "                        \"id\": id,\n",
    "                        \"page_content\": content,\n",
    "                        \"source\": source,\n",
    "                        \"page\": page,\n",
    "                        }\n",
    "            }\n",
    "\n",
    "        metadata = PointStruct(id=id, payload=payload)\n",
    "        chunked_metadata.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TextNode' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m documents \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(input_dir\u001b[38;5;241m=\u001b[39mpath)\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m      4\u001b[0m nodes \u001b[38;5;241m=\u001b[39m  Sentence_Splitter_docs_into_nodes(documents)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mextract_metadata_from_pdf_and_process_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mextract_metadata_from_pdf_and_process_nodes\u001b[1;34m(pdf_path, nodes)\u001b[0m\n\u001b[0;32m     24\u001b[0m chunked_metadata \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[1;32m---> 27\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Assume each node has 'content'\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     source \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     29\u001b[0m     page \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TextNode' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "path = (r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\")\n",
    "pdf_path = (r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\\Adaptive-RAG.pdf\")\n",
    "documents = SimpleDirectoryReader(input_dir=path).load_data()\n",
    "nodes =  Sentence_Splitter_docs_into_nodes(documents)\n",
    "extract_metadata_from_pdf_and_process_nodes(pdf_path, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from fastembed import SparseTextEmbedding, TextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# Load environmental variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "Qdrant_API_KEY = os.getenv('Qdrant_API_KEY')\n",
    "Qdrant_URL = os.getenv('Qdrant_URL')\n",
    "Collection_Name = os.getenv('Collection_Name')\n",
    "\n",
    "class QdrantIndexing:\n",
    "    \"\"\"\n",
    "    A class for indexing documents using Qdrant vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the QdrantIndexing object.\n",
    "        \"\"\"\n",
    "        self.data_path = r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\\nodes.json\"\n",
    "        self.Dense_Embedding_Model = \"jinaai/jina-embeddings-v2-base-en\t\"\n",
    "        self.Sparse_Embedding_Model = \"Qdrant/bm42-all-minilm-l6-v2-attentions\"\n",
    "        self.qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL,\n",
    "                            api_key=Qdrant_API_KEY)\n",
    "        self.qdrant_client.set_model(self.Dense_Embedding_Model)\n",
    "        self.qdrant_client.set_sparse_model(self.Sparse_Embedding_Model)\n",
    "        self.metadata = []\n",
    "        self.documents = []\n",
    "        logging.info(\"QdrantIndexing object initialized.\")\n",
    "\n",
    "    def load_nodes(self, input_file):\n",
    "        \"\"\"\n",
    "        Load nodes from a JSON file and extract metadata and documents.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the JSON file.\n",
    "        \"\"\"\n",
    "        with open(input_file, 'r') as file:\n",
    "            self.nodes = json.load(file)\n",
    "\n",
    "        for node in self.nodes:\n",
    "            self.metadata.append(node['metadata'])\n",
    "            self.documents.append(node['text'])\n",
    "\n",
    "        logging.info(f\"Loaded {len(self.nodes)} nodes from JSON file.\")\n",
    "\n",
    "    def client_collection(self):\n",
    "        \"\"\"\n",
    "        Create a collection in Qdrant vector database.\n",
    "        \"\"\"\n",
    "        if not self.qdrant_client.collection_exists(collection_name=f\"{Collection_Name}\"): \n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name= Collection_Name,\n",
    "                vectors_config={\n",
    "                    \"nomic\": models.VectorParams(\n",
    "                        size = 768,\n",
    "                        distance = models.Distance.COSINE,\n",
    "                        #optimizers_config=models.OptimizersConfigDiff(memmap_threshold=10000),\n",
    "                    )\n",
    "                },\n",
    "                sparse_vectors_config={\n",
    "                    \"bm42\": models.SparseVectorParams(\n",
    "                        modifier = models.Modifier.IDF,\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "            logging.info(f\"Created collection '{Collection_Name}' in Qdrant vector database.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_config= qdrant_client.get_fastembed_vector_params(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fast-bge-small-en': VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)},)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL,\n",
    "                            api_key=Qdrant_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
