{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from llama_index.core.schema import TransformComponent\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import TitleExtractor, SummaryExtractor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "#huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "#embed_model = LangchainEmbedding(huggingface_embeddings)\n",
    "\n",
    "# Define a custom transformation component\n",
    "class CustomTransformation(TransformComponent):\n",
    "    def __call__(self, nodes, **kwargs):\n",
    "        for node in nodes:\n",
    "            node.text = node.text.lower()\n",
    "            node.text = re.sub(r'\\s+', ' ', node.text)  # Replace multiple spaces with a single space\n",
    "            node.text = re.sub(r'[^\\w\\s]', '', node.text)  # Removes punctuation\n",
    "        return nodes\n",
    "\n",
    "# # Define the embedding model transformation component\n",
    "# class EmbeddingModel(TransformComponent):\n",
    "#     def __init__(self):\n",
    "#         self.model = embed_model\n",
    "\n",
    "#     def __call__(self, nodes):\n",
    "#         for node in nodes:\n",
    "#             node.embedding = self.model.get_text_embedding(node.text)\n",
    "#         return nodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "def get_embedding_model(embedding_model_name, embed_batch_size):\n",
    "    embedding_model = HuggingFaceEmbedding(\n",
    "            model_name=embedding_model_name,\n",
    "            embed_batch_size=embed_batch_size\n",
    "        )\n",
    "    return embedding_model\n",
    "\n",
    "class EmbedModel(TransformComponent):\n",
    "    embedding_model: object = Field(default=None, exclude=True)\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            embed_batch_size=100\n",
    "        )\n",
    "\n",
    "    def __call__(self, nodes: List[object]) -> List[object]:\n",
    "        for node in nodes:\n",
    "            node.embedding = self.embedding_model.get_text_embedding(node.text)\n",
    "        return nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentence_Splitter_docs_into_nodes(all_documents):\n",
    "    try:\n",
    "        splitter = SentenceSplitter(\n",
    "            chunk_size=1500,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "\n",
    "        nodes = splitter.get_nodes_from_documents(all_documents)\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents into nodes: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sentence_Splitter_docs_into_nodes() missing 1 required positional argument: 'all_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the ingestion pipeline\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(\n\u001b[0;32m      3\u001b[0m     transformations\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         CustomTransformation(),\n\u001b[1;32m----> 5\u001b[0m         \u001b[43mSentence_Splitter_docs_into_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      6\u001b[0m         EmbedModel(),\n\u001b[0;32m      7\u001b[0m     ]\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Sentence_Splitter_docs_into_nodes() missing 1 required positional argument: 'all_documents'"
     ]
    }
   ],
   "source": [
    "# Create the ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        CustomTransformation(),\n",
    "        Sentence_Splitter_docs_into_nodes(),\n",
    "        EmbedModel(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 5/5 [00:09<00:00,  1.87s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'show_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(transformations\u001b[38;5;241m=\u001b[39m[CustomTransformation(), SentenceSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m),EmbedModel()])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Run the ingestion pipeline\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m nodes_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(nodes_parsed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\ingestion\\pipeline.py:567\u001b[0m, in \u001b[0;36mIngestionPipeline.run\u001b[1;34m(self, show_progress, documents, nodes, cache_collection, in_place, store_doc_text, num_workers, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y, nodes_parallel, [])\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43mrun_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_place\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_place\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39madd([n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\ingestion\\pipeline.py:122\u001b[0m, in \u001b[0;36mrun_transformations\u001b[1;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m cached_nodes\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m         cache\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;28mhash\u001b[39m, nodes, collection\u001b[38;5;241m=\u001b[39mcache_collection)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:221\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;129m@wrapt\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(func, instance, args, kwargs):\n\u001b[1;32m--> 221\u001b[0m     bound_args \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     id_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     token \u001b[38;5;241m=\u001b[39m active_span_id\u001b[38;5;241m.\u001b[39mset(id_)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:3211\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:3200\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3198\u001b[0m         arguments[kwargs_param\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m   3199\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3200\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3201\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   3202\u001b[0m                 arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[0;32m   3204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[1;31mTypeError\u001b[0m: got an unexpected keyword argument 'show_progress'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load data from directory\n",
    "\n",
    "        #reader = SimpleDirectoryReader(input_dir=r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\")\n",
    "        documents = SimpleDirectoryReader(input_dir=r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\").load_data(show_progress = True)\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "        if documents:\n",
    "            documents = CustomTransformation(documents)\n",
    "\n",
    "            # Split documents into nodes\n",
    "            nodes = Sentence_Splitter_docs_into_nodes(documents)\n",
    "\n",
    "            # Initialize embedding model\n",
    "            embeddings = EmbedModel(nodes)\n",
    "        else:\n",
    "            print(\"No documents to process.\")\n",
    "\n",
    "        # Run the ingestion pipeline\n",
    "        #nodes_parsed = pipeline.run(documents=documents)\n",
    "        print(f\"Created {len(embeddings)} nodes\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqdrant_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QdrantClient\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from uuid import uuid4\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentence_Splitter_docs_into_nodes(all_documents):\n",
    "    \"\"\"\n",
    "    Splits the documents into nodes using a sentence splitter.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        splitter = SentenceSplitter(\n",
    "            chunk_size=1500,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "\n",
    "        nodes = splitter.get_nodes_from_documents(all_documents)\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents into nodes: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from uuid import uuid4\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def extract_metadata_from_pdf_and_process_nodes(pdf_path, nodes):\n",
    "    \"\"\"\n",
    "    Extract metadata from a PDF and process the nodes with metadata.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    pdf_path (str): The path to the PDF file.\n",
    "    nodes (list): The list of document nodes.\n",
    "    client (QdrantClient): The Qdrant client instance.\n",
    "    collection_name (str): The name of the collection.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = document.metadata\n",
    "    \n",
    "    # Prepare the chunked metadata list\n",
    "    chunked_metadata = []\n",
    "    \n",
    "    for item in nodes:\n",
    "        content = item['content']  # Assume each node has 'content'\n",
    "        source = item['metadata']['source']\n",
    "        page = item['metadata']['page']\n",
    "        \n",
    "        id = str(uuid4())\n",
    "\n",
    "        payload = {\n",
    "           \"page_content\": content,\n",
    "           \"metadata\": {\n",
    "                        \"id\": id,\n",
    "                        \"page_content\": content,\n",
    "                        \"source\": source,\n",
    "                        \"page\": page,\n",
    "                        \"Title\": metadata.get('title', 'N/A'),\n",
    "                        \"Author\": metadata.get('author', 'N/A'),\n",
    "                        \"CreationDate\": metadata.get('creationDate', 'N/A'),\n",
    "                        }\n",
    "            }\n",
    "\n",
    "        metadata_struct = PointStruct(id=id, payload=payload)\n",
    "        chunked_metadata.append(metadata_struct)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_metadata(data):\n",
    "    \"\"\"\n",
    "    Process and upsert chunked metadata into Qdrant.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    data (list): The list of document chunks.\n",
    "    client (QdrantClient): The Qdrant client instance.\n",
    "    collection_name (str): The name of the collection.\n",
    "\n",
    "    \"\"\"\n",
    "    chunked_metadata = []\n",
    "\n",
    "    for item in data:\n",
    "        content = item.page_content\n",
    "\n",
    "        id = str(uuid4())\n",
    "        source = item.metadata[\"source\"]\n",
    "        page = item.metadata[\"page\"]\n",
    "\n",
    "        payload = {\n",
    "           \"page_content\": content,\n",
    "           \"metadata\": {\n",
    "                        \"id\": id,\n",
    "                        \"page_content\": content,\n",
    "                        \"source\": source,\n",
    "                        \"page\": page,\n",
    "                        }\n",
    "            }\n",
    "\n",
    "        metadata = PointStruct(id=id, payload=payload)\n",
    "        chunked_metadata.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TextNode' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m documents \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(input_dir\u001b[38;5;241m=\u001b[39mpath)\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m      4\u001b[0m nodes \u001b[38;5;241m=\u001b[39m  Sentence_Splitter_docs_into_nodes(documents)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mextract_metadata_from_pdf_and_process_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mextract_metadata_from_pdf_and_process_nodes\u001b[1;34m(pdf_path, nodes)\u001b[0m\n\u001b[0;32m     24\u001b[0m chunked_metadata \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[1;32m---> 27\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Assume each node has 'content'\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     source \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     29\u001b[0m     page \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TextNode' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "path = (r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\")\n",
    "pdf_path = (r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\\Adaptive-RAG.pdf\")\n",
    "documents = SimpleDirectoryReader(input_dir=path).load_data()\n",
    "nodes =  Sentence_Splitter_docs_into_nodes(documents)\n",
    "extract_metadata_from_pdf_and_process_nodes(pdf_path, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from fastembed import SparseTextEmbedding, TextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# Load environmental variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "Qdrant_API_KEY = os.getenv('Qdrant_API_KEY')\n",
    "Qdrant_URL = os.getenv('Qdrant_URL')\n",
    "Collection_Name = os.getenv('Collection_Name')\n",
    "\n",
    "class QdrantIndexing:\n",
    "    \"\"\"\n",
    "    A class for indexing documents using Qdrant vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the QdrantIndexing object.\n",
    "        \"\"\"\n",
    "        self.data_path = r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Using-Hybrid-Search-and-Re-Ranker\\data\\nodes.json\"\n",
    "        self.Dense_Embedding_Model = \"jinaai/jina-embeddings-v2-base-en\t\"\n",
    "        self.Sparse_Embedding_Model = \"Qdrant/bm42-all-minilm-l6-v2-attentions\"\n",
    "        self.qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL,\n",
    "                            api_key=Qdrant_API_KEY)\n",
    "        self.qdrant_client.set_model(self.Dense_Embedding_Model)\n",
    "        self.qdrant_client.set_sparse_model(self.Sparse_Embedding_Model)\n",
    "        self.metadata = []\n",
    "        self.documents = []\n",
    "        logging.info(\"QdrantIndexing object initialized.\")\n",
    "\n",
    "    def load_nodes(self, input_file):\n",
    "        \"\"\"\n",
    "        Load nodes from a JSON file and extract metadata and documents.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the JSON file.\n",
    "        \"\"\"\n",
    "        with open(input_file, 'r') as file:\n",
    "            self.nodes = json.load(file)\n",
    "\n",
    "        for node in self.nodes:\n",
    "            self.metadata.append(node['metadata'])\n",
    "            self.documents.append(node['text'])\n",
    "\n",
    "        logging.info(f\"Loaded {len(self.nodes)} nodes from JSON file.\")\n",
    "\n",
    "    def client_collection(self):\n",
    "        \"\"\"\n",
    "        Create a collection in Qdrant vector database.\n",
    "        \"\"\"\n",
    "        if not self.qdrant_client.collection_exists(collection_name=f\"{Collection_Name}\"): \n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name= Collection_Name,\n",
    "                vectors_config={\n",
    "                    \"nomic\": models.VectorParams(\n",
    "                        size = 768,\n",
    "                        distance = models.Distance.COSINE,\n",
    "                        #optimizers_config=models.OptimizersConfigDiff(memmap_threshold=10000),\n",
    "                    )\n",
    "                },\n",
    "                sparse_vectors_config={\n",
    "                    \"bm42\": models.SparseVectorParams(\n",
    "                        modifier = models.Modifier.IDF,\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "            logging.info(f\"Created collection '{Collection_Name}' in Qdrant vector database.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL,\n",
    "                            api_key=Qdrant_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fast-bge-small-en': VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)},)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_config= qdrant_client.get_fastembed_vector_params(),\n",
    "vectors_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff5b4e54bd8484699e0a94aa5e9e1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([613153351, 74040069], [1048741542, 74040069]) ([0.3703994096830968, 0.3338314745830077], [0.3597222867208645, 0.4026283316035835])\n"
     ]
    }
   ],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "\n",
    "model = SparseTextEmbedding(\n",
    "    model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    "    # if using fastembed-gpu with cuda+onnx installed\n",
    "    # providers=[\"CudaExecutionProvider\"],\n",
    ")\n",
    "\n",
    "embeddings = model.embed([\"hello world\", \"goodbye world\"])\n",
    "\n",
    "indices, values = zip(\n",
    "    *[\n",
    "        (embedding.indices.tolist(), embedding.values.tolist())\n",
    "        for embedding in embeddings\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(indices, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_vector(self, text):\n",
    "        \"\"\"\n",
    "        Create a sparse vector from the text using SPLADE.\n",
    "        \"\"\"\n",
    "        # Generate the sparse vector using SPLADE model\n",
    "        embeddings = self.sparse_embedding_model.embed([text])[0]\n",
    "        sparse_vector = models.SparseVector(\n",
    "            indices=embeddings.indices,\n",
    "            values=embeddings.values\n",
    "        )\n",
    "        return sparse_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "class reranking():\n",
    "    def __init__(self) -> None:\n",
    "        # Load the CrossEncoder model\n",
    "        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    def rerank_documents(self, query, documents):\n",
    "        # Compute the similarity scores between the query and each document\n",
    "        scores = self.model.predict([(query, doc) for doc in documents])\n",
    "\n",
    "        # Sort the documents based on their similarity scores\n",
    "        ranked_documents = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select the top 3 documents\n",
    "        top_documents = [doc for doc, score in ranked_documents[:3]]\n",
    "\n",
    "        return top_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "from rerank import reranking\n",
    "from Retriever import Hybrid_search\n",
    "from llama_index.llms.groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryPipelineBuilder:\n",
    "    def __init__(self):\n",
    "        self.search = Hybrid_search()\n",
    "        self.reranker = reranking()\n",
    "\n",
    "        self.prompt_str = \"\"\"You are an AI assistant specializing in explaining complex topics related to AI-powered RAG systems. Your task is to provide a clear, concise, and informative explanation based on the following context and query.\n",
    "\n",
    "            Context:\n",
    "            {context_str}\n",
    "\n",
    "            Query: {query_str}\n",
    "\n",
    "            Please follow these guidelines in your response:\n",
    "            1. Start with a brief overview of the concept mentioned in the query.\n",
    "            2. Provide at least one concrete example or use case to illustrate the concept.\n",
    "            3. If there are any limitations or challenges associated with this concept, briefly mention them.\n",
    "            4. Conclude with a sentence about the potential future impact or applications of this concept.\n",
    "\n",
    "            Your explanation should be informative yet accessible, suitable for someone with a basic understanding of AI and RAG. If the query asks for information not present in the context, please state that you don't have enough information to provide a complete answer, and only respond based on the given context.\n",
    "\n",
    "            Response:\n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "    def query_pipeline(self, query: str, filename: str) -> str:\n",
    "        llm = Groq(model=\"mixtral-8x7b-32768\", api_key=os.getenv('groq_api_key'))\n",
    "        prompt_tmpl = PromptTemplate(self.prompt_str)\n",
    "\n",
    "        metadata_filter = self.search.metadata_filter(filename)\n",
    "        results = self.search.query_hybrid_search(query, metadata_filter)\n",
    "\n",
    "        reranked_documents = self.reranker.rerank_documents(query, results)\n",
    "        summarizer = TreeSummarize(llm=llm)\n",
    "        \n",
    "        # Modify the context to be a single string\n",
    "        context = \"\".join(reranked_documents)\n",
    "        prompt = prompt_tmpl.format(context_str=context, query_str=query)\n",
    "\n",
    "        pipeline = QueryPipeline()\n",
    "        pipeline.add_modules({\n",
    "            \"prompt\": prompt,\n",
    "            \"llm\": llm,\n",
    "            \"summarizer\": summarizer\n",
    "        })\n",
    "\n",
    "        pipeline.add_link(\"prompt\", \"llm\")\n",
    "        pipeline.add_link(\"llm\", \"summarizer\")\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce28a01c4d2460395862ebebbcd43d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f417b2d6bc294f7fa1b41f1ccfd25b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n",
      "INFO:httpx:HTTP Request: POST https://c77ac75e-3a41-4acc-98d2-c9c3eb11b5ea.us-east4-0.gcp.cloud.qdrant.io:6333/collections/RAG-Hybrid-Search-with-Reranking/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea9e1d3f9f045ccaf6a1ac26814e5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, got PromptTemplate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     pipeline_builder \u001b[38;5;241m=\u001b[39m QueryPipelineBuilder()\n\u001b[1;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExplain adaptive retrieval and its advantages.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdaptive-RAG.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[14], line 44\u001b[0m, in \u001b[0;36mQueryPipelineBuilder.query_pipeline\u001b[1;34m(self, query, filename)\u001b[0m\n\u001b[0;32m     37\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m TreeSummarize(llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[0;32m     39\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m QueryPipeline(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     40\u001b[0m pipeline\u001b[38;5;241m.\u001b[39madd_modules({\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata_filter,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m\"\u001b[39m: results,\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreranker\u001b[39m\u001b[38;5;124m\"\u001b[39m: reranked_documents,\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mPromptTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_tmpl\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m: llm,\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: summarizer\n\u001b[0;32m     47\u001b[0m })\n\u001b[0;32m     49\u001b[0m pipeline\u001b[38;5;241m.\u001b[39madd_link(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m pipeline\u001b[38;5;241m.\u001b[39madd_link(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreranker\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\prompts\\base.py:154\u001b[0m, in \u001b[0;36mPromptTemplate.__init__\u001b[1;34m(self, template, prompt_type, output_parser, metadata, template_var_mappings, function_mappings, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    152\u001b[0m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_type\n\u001b[1;32m--> 154\u001b[0m template_vars \u001b[38;5;241m=\u001b[39m \u001b[43mget_template_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    157\u001b[0m     template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[0;32m    158\u001b[0m     template_vars\u001b[38;5;241m=\u001b[39mtemplate_vars,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m     function_mappings\u001b[38;5;241m=\u001b[39mfunction_mappings,\n\u001b[0;32m    164\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\prompts\\utils.py:12\u001b[0m, in \u001b[0;36mget_template_vars\u001b[1;34m(template_str)\u001b[0m\n\u001b[0;32m      9\u001b[0m variables \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m formatter \u001b[38;5;241m=\u001b[39m Formatter()\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, variable_name, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate_str\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable_name:\n\u001b[0;32m     14\u001b[0m         variables\u001b[38;5;241m.\u001b[39mappend(variable_name)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\Lib\\string.py:288\u001b[0m, in \u001b[0;36mFormatter.parse\u001b[1;34m(self, format_string)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string):\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _string\u001b[38;5;241m.\u001b[39mformatter_parser(format_string)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, got PromptTemplate"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pipeline_builder = QueryPipelineBuilder()\n",
    "    response = pipeline_builder.query_pipeline(query=\"Explain adaptive retrieval and its advantages.\", filename=\"Adaptive-RAG.pdf\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "from rerank import reranking\n",
    "from Retriever import Hybrid_search\n",
    "from llama_index.llms.groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"You are an AI assistant specializing in explaining complex topics related to AI-powered RAG systems. Your task is to provide a clear, concise, and informative explanation based on the following context and query.\n",
    "\n",
    "        Context:\n",
    "        {context_str}\n",
    "\n",
    "        Query: {query_str}\n",
    "\n",
    "        Please follow these guidelines in your response:\n",
    "        1. Start with a brief overview of the concept mentioned in the query.\n",
    "        2. Provide at least one concrete example or use case to illustrate the concept.\n",
    "        3. If there are any limitations or challenges associated with this concept, briefly mention them.\n",
    "        4. Conclude with a sentence about the potential future impact or applications of this concept.\n",
    "\n",
    "        Your explanation should be informative yet accessible, suitable for someone with a basic understanding of AI and RAG. If the query asks for information not present in the context, please state that you don't have enough information to provide a complete answer, and only respond based on the given context.\n",
    "\n",
    "        Response:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = Groq(model=\"mixtral-8x7b-32768\", api_key=os.getenv('groq_api_key'))\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b9f773781c4c85b001c64ce1bdd932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8061a7cc1449ac9a74b455628e333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    " search = Hybrid_search()\n",
    "reranker = reranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Explain adaptive retrieval and its advantages.\"\n",
    "filename=\"Adaptive-RAG.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://c77ac75e-3a41-4acc-98d2-c9c3eb11b5ea.us-east4-0.gcp.cloud.qdrant.io:6333/collections/RAG-Hybrid-Search-with-Reranking/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metadata_filter = search.metadata_filter(filename)\n",
    "results = search.query_hybrid_search(query, metadata_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313eb9bbee6240a0a79c9c3194df246a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "reranked_documents = reranker.rerank_documents(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = \"\".join(reranked_documents)\n",
    "prompt = prompt_tmpl.format(context_str=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an AI assistant specializing in explaining complex topics related to AI-powered RAG systems. Your task is to provide a clear, concise, and informative explanation based on the following context and query.\\n\\n        Context:\\n        adaptiverag learning to adapt retrievalaugmented large language models through question complexity soyeong jeong1jinheon baek2sukmin cho1sung ju hwang12jong c park1 school of computing1graduate school of ai2 korea advanced institute of science and technology12 starsuzijinheonbaeknelllpicsjhwang82jongparkkaistackr abstract retrievalaugmented large language models llms which incorporate the nonparametric knowledge from external knowledge bases into llms have emerged as a promising approach to enhancing response accuracy in several tasks such as questionanswering qa however even though there are various approaches deal ing with queries of different complexities they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multistep queries yet not all user requests fall into only one of the sim ple or complex categories in this work we propose a novel adaptive qa framework that can dynamically select the most suitable strat egy for retrievalaugmented llms from the simplest to the most sophisticated ones based on the query complexity also this selec tion process is operationalized with a classi fier which is a smaller lm trained to predict the complexity level of incoming queries with automatically collected labels obtained from actual predicted outcomes of models and in herent inductive biases in datasets this ap proach offers a balanced strategy seamlessly adapting between the iterative and singlestep retrievalaugmented llms as well as the no retrieval methods in response to a range of query complexities we validate our model on a set of opendomain qa datasets cov ering multiple query complexities and show that ours enhances the overall efficiency and accuracy of qa systems compared to rele vant baselines including the adaptive retrieval approaches code is available at https githubcomstarsuziadaptiverag  1 introduction recent large language models llms brown et al 2020 openai 2023 touvron et al 2023 anil et al 2023 have shown overwhelming per formances across diverse tasks including question corresponding author 05 10 15 20 25 30 35 time per query4748495051performance f1 no retrieval singlestep approach adaptive retrieval multistep approach adaptiverag oursperformance vs time with gpt35figure 1 qa performance f1 and efficiency timequery for different retrievalaugmented generation approaches we use the gpt35turboinstruct as the base llm answering qa yang et al 2018 kwiatkowski et al 2019 however they still generate factu ally incorrect answers since their knowledge solely relies on their parametric memory kasai et al 2022 mallen et al 2023 meanwhile memoriz ing all the everchanging world knowledge may not be possible to address this problem retrieval augmented llms borgeaud et al 2022 izacard et al 2023 shi et al 2023 which incorporate nonparametric knowledge into llms with addi tional retrieval modules have gained much increas ing attention specifically these models access a knowledge base which serves as an extensive repository of information across various subjects and disciplines to retrieve information relevant to the given input and then incorporate the retrieved information into llms which enables them to stay accurate and current with the world knowledge a particularly salient application of retrieval augmented llms is to handling qa tasks whose goal is to provide correct answers in response to user queries especially those of high complexity early work on retrievalaugmented llms focuses primarily on singlehop queries lazaridou et al 2022 ram et al 2023 whose answers are typ ically found within a single document therefore this approach involves retrieving a relevant doc ument based on the query and subsequently inte grating this information into qa models to formu late a response however unlike this singlehop qa some queries require connecting and aggregat ing multiple documents which are furthermorearxiv240314403v2 cscl 28 mar 2024table 5 case study with gpt35 turbo where we represent the factual error in red and the accurate information in blue dataset question adaptive retrieval adaptiverag ours nq singlehopwhich famous corpo rate logo changed to a flat colourcolor sans serif font in its first ma jor change since 1999query type b singlestep approach reasoning the famous corporate logo that changed to a flat color sansserif font in its first major change since 1999 is microsoft microsoft changed its logo in 2012 replacing the italicized multicolored font with a simple flat grey font so the answer is microsoftquery type a non retrieval reasoning the famous corporate logo that changed to a flat color sans serif font in its first ma jor change since 1999 is google google changed its logo in 2015 so the answer is google musique multihopwho is the child of the italian navigator who explored the eastern coast of the continent césar gaytan was born in for the englishquery type a non retrieval reasoning césar gaytan was born in the conti nent of north america the italian navigator who explored the eastern coast of north america for the english was giovanni caboto also known as john cabot so the answer is giovanni cabotojohn cabotquery type c multistep approach reasoning césarr gaytan was born in guadalajara jalisco mexico the italian navigator who explored the eastern coast of the continent for the english is john cabot john cabots son is sebastian cabot so the answer is sebastian cabot table 6 results with varying model sizes for classifiers qa classifier accuracy sizes f1 step all no one multi small 60m 4583 964 5348 2665 7062 5318 base 223m 4597 983 5341 2642 6946 5682 large 770m 4694 1084 5452 3052 6628 6545 adaptive retrieval fetches additional documents leading to longer processing times and occasion ally producing incorrect responses due to the inclu sion of partially irrelevant information about mi crosoft meanwhile faced with a complex ques tion adaptiverag seeks out relevant information including details like a son of john cabot which may not have been stored in llms while adaptive retrieval fails to request such information from external sources resulting in inaccurate answers 6 conclusion in this work we proposed the adaptive retrieval augmented generation framework referred to as adaptiverag to handle queries of various complexities specifically adaptiverag is de signed to dynamically adjust its query handling strategies in the unified retrievalaugmented llm based on the complexity of queries that they en counter which spans across a spectrum of the non retrievalbased approach for the most straightfor ward queries to the singlestep approach for the queries of moderate complexity and finally to the multistep approach for the complex queries the core step of our adaptiverag lies in determin ing the complexity of the given query which is instrumental in selecting the most suitable strat egy for its answer to operationalize this process we trained a smaller language model with query complexity pairs which are automatically anno tated from the predicted outcomes and the inductive biases in datasets we validated our adaptiveragon a collection of opendomain qa datasets cover ing the multiple query complexities including both the single and multihop questions the results demonstrate that our adaptiverag enhances the overall accuracy and efficiency of qa systems al locating more resources to handle complex queries while efficiently handling simpler queries com pared to the existing onesizefitsall approaches that tend to be either minimalist or maximalist over varying query complexities limitations while our adaptiverag shows clear advantages in effectiveness and efficiency by determining the query complexity and then leveraging the most suitable approach for tackling it it is important to recognize that there still exist potential avenues for improving the classifier from the perspectives of its training datasets and architecture specifi cally as there are no available datasets for training the querycomplexity classifier we automatically create new data based on the model prediction out comes and the inductive dataset biases however our labeling process is one specific instantiation of labeling the query complexity and it may have the potential to label queries incorrectly despite its effectiveness therefore future work may create new datasets that are annotated with a diverse range of query complexities in addition to the labels of questionanswer pairs also as the performance gap between the ideal classifier in table 1 and the current classifier in figure 3 indicates there is still room to improve the effectiveness of the classifier in other words our classifier design based on the smaller lm is the initial simplest instantiation for classifying the query complexity and based upon it future work may improve the classifier archi tecture and its performance which will positively contribute to the overall qa performanceour contributions and findings are threefold we point out the realistic scenario of queries of varying complexities and find out that existing retrievalaugmented generation approaches tend to be overly simple or complex we adapt retrievalaugmented llms to the query complexity assessed by the classifier which en ables the utilization of the most suitable approach tailored to each query we show that our adaptiverag is highly effec tive and efficient balancing between the com plexity and the simplicity for diverse queries 2 related work opendomain qa opendomain qa is the task of accurately answering a query by sourcing for queryrelevant documents and then interpreting them to provide answers chen et al 2017 zhu et al 2021 which thus generally involves two modules a retriever karpukhin et al 2020 xiong et al 2021 and a reader yang et al 2019 izac ard and grave 2021 jeong et al 2023 along with the emergence of llms with superior rea soning capabilities thanks to their billionsized pa rameters wei et al 2022a a synergy between llms and retrievers has led to significant advance ments lazaridou et al 2022 ram et al 2023 specifically this integration has been shown to enhance opendomain qa by mitigating the hallu cination problem from llms through strengthened reasoning abilities of the reader as well as utiliz ing the retrieved external documents cho et al 2023 despite these advancements for singlehop retrievalaugmented llms however the complex ity of some queries needs a more complex strategy multihop qa multihop qa is an extension of conventional opendomain qa which addition ally requires the system to comprehensively gather and contextualize information from multiple docu ments often iteratively to answer more complex queries trivedi et al 2022a yang et al 2018 in the realm of multihop qa the approach to itera tively access both llms and the retrieval module is generally employed specifically khattab et al 2022 press et al 2023 pereira et al 2023 and khot et al 2023 proposed to first decom pose the multihop queries into simpler singlehop queries repeatedly access the llms and retriever to solve these subqueries and merge their solu tions to formulate a complete answer in contrastto this decompositionbased approach other re cent studies such as yao et al 2023 and trivedi et al 2023 explored the interleaving of chainof thought reasoning wei et al 2022b  a method where a logical sequence of thoughts is generated  with document retrieval repeatedly applying this process until the reasoning chain generates the an swer in addition jiang et al 2023 introduced an approach to repeatedly retrieving new documents if the tokens within generated sentences have low confidence however the aforementioned methods overlooked the fact that in realworld scenarios queries are of a wide variety of complexities there fore it would be largely inefficient to iteratively access llms and retrievers for every query which might be simple enough with a single retrieval step or even only with an llm itself adaptive retrieval to handle queries of varying complexities the adaptive retrieval strategy aims to dynamically decide whether to retrieve documents or not based on each querys complexity in this vein mallen et al 2023 proposed to decide the querys complexity level based on the frequency of its entities and suggested using the retrieval mod ules only when the frequency falls below a cer tain threshold however this approach focusing solely on the binary decision of whether to retrieve or not may not be sufficient for more complex queries that require multiple reasoning steps ad ditionally qi et al 2021 proposed an approach that performs a fixed set of operations retrieving reading and reranking multiple times until the an swer is derived for the given query which is built upon traditional bertlike lms however unlike our adaptiverag which predetermines the query complexity and adapts the operational behavior of any offtheshelf llms accordingly this approach applies the same fixed operations to every query regardless of its complexity but also necessitates additional specific training to lms concurrent to our work asai et al 2024 suggested training a so phisticated model to dynamically retrieve critique and generate the text nevertheless we argue that all the aforementioned adaptive retrieval methods that rely on a single model might be suboptimal in handling a variety of queries of a range of differ ent complexities since they tend to be either overly simple or complex for all the input queries which demands a new approach that can select the most suitable strategy of retrievalaugmented llms tai lored to the query complexity\\n\\n        Query: Explain adaptive retrieval and its advantages.\\n\\n        Please follow these guidelines in your response:\\n        1. Start with a brief overview of the concept mentioned in the query.\\n        2. Provide at least one concrete example or use case to illustrate the concept.\\n        3. If there are any limitations or challenges associated with this concept, briefly mention them.\\n        4. Conclude with a sentence about the potential future impact or applications of this concept.\\n\\n        Your explanation should be informative yet accessible, suitable for someone with a basic understanding of AI and RAG. If the query asks for information not present in the context, please state that you don't have enough information to provide a complete answer, and only respond based on the given context.\\n\\n        Response:\\n        \""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TreeSummarize(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = QueryPipeline(verbose=True)\n",
    "pipeline.add_modules({\n",
    "    \"prompt\": PromptTemplate(prompt),\n",
    "    \"llm\": llm,\n",
    "    \"summarizer\": summarizer\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline.add_link(\"prompt\", \"llm\")\n",
    "pipeline.add_link(\"llm\", \"summarizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module prompt with input: \n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: You are an AI assistant specializing in explaining complex topics related to AI-powered RAG systems. Your task is to provide a clear, concise, and informative explanation based on the following contex...\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Module input keys must have exactly one key if dest_key is not specified. Remaining keys: in module: {'nodes', 'query_str'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\query_pipeline\\query.py:410\u001b[0m, in \u001b[0;36mQueryPipeline.run\u001b[1;34m(self, return_values_direct, callback_manager, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m     query_payload \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mstr\u001b[39m(kwargs))\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    408\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_payload}\n\u001b[0;32m    409\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 410\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_values_direct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_values_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\query_pipeline\\query.py:774\u001b[0m, in \u001b[0;36mQueryPipeline._run\u001b[1;34m(self, return_values_direct, show_intermediates, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_outputs, intermediates\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     result_outputs, intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43mroot_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_intermediates\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_result_output(result_outputs, return_values_direct),\n\u001b[0;32m    780\u001b[0m         intermediates,\n\u001b[0;32m    781\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\query_pipeline\\query.py:935\u001b[0m, in \u001b[0;36mQueryPipeline._run_multi\u001b[1;34m(self, module_input_dict, show_intermediates)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    928\u001b[0m         show_intermediates\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m module_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m run_state\u001b[38;5;241m.\u001b[39mintermediate_outputs\n\u001b[0;32m    930\u001b[0m     ):\n\u001b[0;32m    931\u001b[0m         run_state\u001b[38;5;241m.\u001b[39mintermediate_outputs[module_key] \u001b[38;5;241m=\u001b[39m ComponentIntermediates(\n\u001b[0;32m    932\u001b[0m             inputs\u001b[38;5;241m=\u001b[39mmodule_input, outputs\u001b[38;5;241m=\u001b[39moutput_dict\n\u001b[0;32m    933\u001b[0m         )\n\u001b[1;32m--> 935\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_component_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m next_module_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_next_module_keys(\n\u001b[0;32m    942\u001b[0m     run_state,\n\u001b[0;32m    943\u001b[0m )\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m next_module_keys:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\query_pipeline\\query.py:872\u001b[0m, in \u001b[0;36mQueryPipeline.process_component_output\u001b[1;34m(self, output_dict, module_key, run_state)\u001b[0m\n\u001b[0;32m    869\u001b[0m             dest_output \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m attr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m](output):\n\u001b[1;32m--> 872\u001b[0m             \u001b[43madd_output_to_module_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdest_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdest_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_module_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m run_state\u001b[38;5;241m.\u001b[39mexecuted_modules\u001b[38;5;241m.\u001b[39madd(module_key)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\llama_index\\core\\query_pipeline\\query.py:86\u001b[0m, in \u001b[0;36madd_output_to_module_inputs\u001b[1;34m(dest_key, output, module, module_inputs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# ensure that there is only one remaining key given partials\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(free_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule input keys must have exactly one key if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdest_key is not specified. Remaining keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min module: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfree_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m         )\n\u001b[0;32m     91\u001b[0m     module_inputs[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(free_keys))] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Module input keys must have exactly one key if dest_key is not specified. Remaining keys: in module: {'nodes', 'query_str'}"
     ]
    }
   ],
   "source": [
    "\n",
    "response = pipeline.run()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
